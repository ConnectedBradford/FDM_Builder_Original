{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e55440-899f-47fa-a7ac-af48b54648db",
   "metadata": {},
   "source": [
    "# FDMBuilder - The Extras\n",
    "\n",
    "If you've arrived here, hopefully you're already comfortable with the basics of the `FDMBuilder` API and have played aroudnd with the `FDMTable` and `FDMDataset` classes a bit. If not, take a look at `fdm_builder_basics_tutorial` and then come back here when you're ready.\n",
    "\n",
    "This tutorial will cover the extra methods and helpers that can be found in the FDMBuilder library. Hopefully, they can help you speed up your FDM building workflow even more. We'll start by taking a look at each of the tools/methods/helpers first, followed by a (hopefully) motivating example that will demonstrate how these tools/methods/helpers can be used \"in the wild\".\n",
    "\n",
    "As with the basics tutorial, we'll start by loading the `FDMBuilder` libraries and create a `DATASET_ID` variable for your test dataset, to make the example code below a little easier to run. Same as before, don't specify a dataset with any tables in it that you'll miss, because we're about to delete them! Replace the `YOUR DATASET HERE` text with the id of your test dataset, and then run the below cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7058a-00c8-43b2-a1da-17d76189d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FDMBuilder.FDMTable import *\n",
    "from FDMBuilder.FDMDataset import *\n",
    "from FDMBuilder.testing_helpers import *\n",
    "\n",
    "### !!REPLACE THIS TEXT!! ###\n",
    "\n",
    "DATASET_ID = \"CY_SAM_TEST\"\n",
    "\n",
    "###\n",
    "\n",
    "# Leave this bit alone!\n",
    "if check_dataset_exists(DATASET_ID):\n",
    "    clear_dataset(DATASET_ID)\n",
    "    print(\"Good to go!\")\n",
    "else:\n",
    "    print(\"#\" * 33 + \" PROBLEM!! \" + 33 * \"#\" + \"\\n\")\n",
    "    print(\"Something doesn't look right. Check you spelled everything correctly,\\n\" \n",
    "          \"your dataset has been created in GCP, and you have the right permisssions\\n\")\n",
    "    print(\"#\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968b109-b9a4-484d-925b-e61fbd8b696c",
   "metadata": {},
   "source": [
    "## BigQuery Cell Magics\n",
    "\n",
    "The first tool isn't anything the FDM pipeline can take credit for. Packaged in the python bigquery library is a \"cell magic\", that allows you to run pure SQL queries directly from a Jupyter notebook cell. A cell magic is a little bit of syntax that changes or adds extra functionality to a notebook cell - the general syntax of a cell magic is `%%magic-name`, so the bigquery cell magic is `%%bigquery`. Add that to the top of a cell, write your SQL below, and juypter/python will do the rest. Give the below a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841e84e-c3c5-496b-88e5-fb50d11975a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT *\n",
    "FROM `CY_FDM_MASTER.person`\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3cd18-2f5c-4827-a776-006d8e524b71",
   "metadata": {},
   "source": [
    "Easy. \n",
    "\n",
    "For those familiar with the pandas library, you can store the results of your query as a `pandas.DataFrame` by naming it immediately after the `%%bigquery` magic. So the following cell runs the same query as above, and stores the result in `eg_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f9318-6bda-4ca0-9e1d-c587542e2c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery eg_df\n",
    "SELECT *\n",
    "FROM `CY_FDM_MASTER.person`\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c439c183-74e5-45c7-b7ea-9e08965c32fb",
   "metadata": {},
   "source": [
    "You can then run the following to take a look at the contents of `eg_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb533bd-b422-4d39-99ef-a76bb942d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da048a-a525-40ff-83e5-61ab29d50f65",
   "metadata": {},
   "source": [
    "If so inclined, you can run and document your SQL pipelines in a notebook by using the above cell magics, and then documenting your work in markdown text cells (like this). Be sure to only stick SQL cells marked with the `%%bigquery` magic - everything inside these cells is interpreted as SQL, so you'll get some pretty colourful errors if you try sticking python in there too.\n",
    "\n",
    "Now on to the extra bits of the FDM pipeline. \n",
    "\n",
    "## FDMTable Helpers\n",
    "\n",
    "The `FDMTable` comes with a bunch of extra \"methods\" that quicken up some of the more \"fiddly\" bits of the BigQuery SQL environment. We'll be using the last of the test tables `test_table_3` to try out these new helpers. By now, it hopefully won't be a shock to see us begin by initialising an `FDMTable` with the location of `test_table_3` and our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724ae23-8fa0-4a77-a671-57660eaa55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3 = FDMTable(\n",
    "    source_table_id = \"CY_FDM_BUILDER_TESTS.test_table_3\",\n",
    "    dataset_id=DATASET_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082517d3-cb8e-4b91-bc9b-9ee16287ed7f",
   "metadata": {},
   "source": [
    "### copy_table_to_dataset\n",
    "\n",
    "In your adventures with the `.build()` process, you may have noticed that the first stage is copying the source table into the FDM dataset. This is a requirement before you start manipulating a table - you don't want to be messing about with the original copy of the data in GCP. Oh no. We need a pristine copy of the original source data, in case anything goes wrong.\n",
    "\n",
    "Based on this, none of the methods below will work if you don't first make a copy of your source table in your FDM dataset - otherwise there's no table in GCP for your `FDMTable` to manipulate. You can quickly create a copy of the source table using the `copy_table_to_dataset` method like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd06576-2704-4e0f-9033-a38da0aeaad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3.copy_table_to_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b77112f-a0f9-409d-99e6-3c0318e4012f",
   "metadata": {},
   "source": [
    "As you'd expect, if you pop over to your GCP SQL workspace, you'll find a fresh copy of `test_table_3` in your test dataset. Pretty simple. Now you can begin playing around with some of the helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14bdb5-b7ae-413c-95bc-4fa113010471",
   "metadata": {},
   "source": [
    "### head\n",
    "\n",
    "If you want a quick reminder of the contents of your `FDMTable` you can call the `head` function - by default it will return a dataframe with the first 10 rows of your table, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571838e-c904-4dc6-aa6d-8c7ec2768ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1fd60-fa82-49f6-b813-6adb3a99d405",
   "metadata": {},
   "source": [
    "If, for some reason, you'd like to see a different number of rows, you can use the `n` argument like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6145e6-5fe9-417a-b315-b482952b7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0928c0-4b83-470d-b256-d851f40779f7",
   "metadata": {},
   "source": [
    "simple.\n",
    "\n",
    "### rename_columns\n",
    "\n",
    "Next is `rename_columns` - you'll be shocked to hear this renames columns in your table. This is surprisingly awkward to do in BigQuery SQL syntax. Hopefully you'll find this a little easier. The logic is as follows:\n",
    "\n",
    "`rename_columns` takes one argument - a python \"dictionary\". Dictionaries are a data type in python that comine a set of \"keys\" with a set of \"values\". They look like this:\n",
    "\n",
    "```\n",
    "    example_dict = {\n",
    "        \"key_1\": \"value_1\",\n",
    "        \"key_2\": \"value_2\",\n",
    "        \"key_3\": \"value_3\",\n",
    "        ...\n",
    "    }\n",
    "```\n",
    "\n",
    "Dictionaries are defined inside curly braces - `{}` - inside which are \"keys\" and \"values\" separated by a colon - `:` - and each key-value pair is separated by a comma - `,`. The input to `rename_columns` is a dictionary where each key is an existing column name that you want to change, and each value is the new name, like so:\n",
    "\n",
    "```\n",
    "    example_rename_columns_input = {\n",
    "        \"old_name_1\": \"new_name_1\",\n",
    "        \"old_name_2\": \"new_name_2\",\n",
    "        \"old_name_3\": \"new_name_3\",\n",
    "        ...\n",
    "    }\n",
    "```\n",
    "\n",
    "So we can rename the `some_data` column in our `test_table_3` to `some_new_data` by running the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f921cd2-d9f0-48f7-b147-bde1bbd81d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3.rename_columns({\"some_data\": \"some_new_data\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a06cf-3751-41a2-b57d-817f5cab5bbc",
   "metadata": {},
   "source": [
    "and we can check that worked using our new found `head` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe3bf2-a139-4390-a83f-7f066f5a9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4d896-e4d7-4cb9-8853-527830f5dc4b",
   "metadata": {},
   "source": [
    "### add_column\n",
    "\n",
    "Next on the list is `add_column`. Shockingly, this addes a new column to our Table. It takes one argument, a string that should look like one column of your standard `SELECT` statement. So, for example:\n",
    "\n",
    "    \"some_new_data * 100 AS some_new_data_x_100\"\n",
    "    \"LENGTH(education_reference) AS ed_ref_length\"\n",
    "    \"LOWER(educatoin_reference) AS lower_case_ed_ref\"\n",
    "\n",
    "If you could stick it at the start of a select statement, it'll work in `add_column`. If you're wondering what any of the above will do, given them a try in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e45bf-95d3-4a15-934b-e97ddd56dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3.add_column(\"some_new_data * 100 AS new_data_x_100\")\n",
    "\n",
    "test_table_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6dec2-f612-4e36-a1d1-69678ad6f904",
   "metadata": {},
   "source": [
    "### drop_column\n",
    "\n",
    "This one couldn't be easier - `drop_column` takes one argument, a column name, and drops/deletes the named column. Give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41fef3-d89a-4e20-888a-ca7254e6a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3.drop_column(\"some_new_data\")\n",
    "\n",
    "test_table_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee970d1c-42d8-465d-bd9b-3d5d44265d74",
   "metadata": {},
   "source": [
    "### quick_build\n",
    "\n",
    "When you were running through the basics of the FDM API, you may have thought the `build` process was a bit lengthy - what with all the long-winded explanations and requests for input. That's where `quick_build` comes in. It's basically a more \"programatic\" way of completing the build process. It takes the training wheels off, so to speak, and in doing so makes the process a lot \"snappier\".\n",
    "\n",
    "`quick_build` takes up to five arguments:\n",
    "\n",
    "* `fdm_start_date_cols`: Either a string or a list, detailing the columns that contain the start date information. If the start date is found in a single column with a datetime or a string that can easily be parsed, then the input would be a string with the column name. If the start date is in multiple columns with individual year, month and day, the input would be a list with the column names - or, a static value for one or more of the year/month/day. If we think back to the test examples, for `test_table_1` we would use `\"start_date\"` and for `test_table_2` we would input `[\"start_year\", \"start_month\", \"15\"]`\n",
    "* `fdm_start_date_format`: This is one of `\"YMD\"`/`\"YDM\"`/`\"DMY\"`/`\"MDY\"`. Hopefully fairly self explanatory. Simply the date format of the start date data. This is required both if you input a single column or multiple columns - so it doesn't matter what order you input your `fdm_start_date_cols` provided you correctly specify the date format.\n",
    "* `fdm_end_date_cols`: This is an optional argument, depending on the need for an end date in the source data you're FDMing. It takes exactly the same input format as the `fdm_start_date_cols` argument, so no need to go over that again.\n",
    "* `fdm_end_date_format`: Again, an optional argument depending on the presence of an end date, with the same input specification as the `fdm_start_date_format`\n",
    "* `verbose`: By default this is set to `True`, and controls the console output while the `quick_build` process is running. When set to `True`, the console will output text telling you what stage the script has reached. If set to `False`, the console output is surpressed.\n",
    "\n",
    "That may seem a little daunting to take in all at once, but the process is really pretty simple once you get started. We'll save working examples for the moment, as we'll see plenty when we take a look at an example workflow below.\n",
    "\n",
    "### recombine\n",
    "\n",
    "You'll hopefully recall that, as part of the `FDMDataset`'s `build` process, the tool splits any entries that are found to have \"problems\" into separate tables. This is an important part of completing an FDM dataset, but presents an issue if you then want to start manipulating the source data after the build - you have two separate tables that contain the source data! This could quickly lead to errors, particularly if you want to correct any of the problems identified during the FDM build.\n",
    "\n",
    "`recombine` is a method designed to resolve this issue. If you want to start manipulating a table that has been split from it's problematic entries, you must first `recombine` it - stich the two tables back together. The method itself couldn't be easier to use - simply call `your_table.recombine()` on a table that has an associated \"fdm_problems\" table, and the script does the rest. \n",
    "\n",
    "You'll find that if you try and use any of the above helpers on a table that has a separate \"fdm_problems\" table or you try to build an FDM from a dataset containing \"fmd_problems\" tables, the method will return an error and ask you to first recombine the problem entries before continuning.  To help in any efforts to correct problem entries, the \"problem\" column from the associated \"fdm_problems\" table is kept after using `recombine`, and is `NULL` for any entries that don't have an associated problem. \n",
    "\n",
    "It would be a bit of a faff setting up an example here, but there will be examples of `recombine` in the workflow below.\n",
    "\n",
    "## Other Helpers\n",
    "\n",
    "Before we take a look at a more involved example workflow, a quick mention of a couple of functions we've already seen but have glossed over until now. These aren't methods attached to either the `FDMTable` or `FDMDataset` classes, but stand-alone functions in their own right. They're all very simple to use:\n",
    "\n",
    "### check_dataset_exists / check_table_exists\n",
    "\n",
    "Both do what they say on the tin - either checking a table or a dataset exists! Simply stick the id of a table or dataset in the function, and it will return either `True` or `False` depending on the existence of the named table/dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae7c94-9b52-4420-b6d0-2290aeef47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dataset_exists(\"CY_FDM_MASTER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0724c9-56cf-4bb0-89fa-18feb54cdc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_table_exists(\"this_table.doesn't.exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4024be-1f2a-458e-858c-2d31a842a5b5",
   "metadata": {},
   "source": [
    "### clear_dataset\n",
    "\n",
    "Again, does pretty much what it says on the tin. `clear_dataset` will remove every table from the dataset you point it at. Obviously, to be used with caution! We'll need to clear out our test dataset before we can start with the example workflow below, so let's do just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7106e6dd-f9e7-4045-a6de-50f677e388ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_dataset(DATASET_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a38ee4-3c70-40de-a739-41582263ad91",
   "metadata": {},
   "source": [
    "## Example FDM Building Workflow\n",
    "\n",
    "Righty, we've reached the now much talked about example workflow. In this example, we'll assume we're building an FDM from scratch, using the tables in `CY_FDM_BUILDER_TESTS`. The process will be similar to that of the basics tutorial, but a little more \"programatic\" and with less navigating backwards and forwards between jupyter and GCP.\n",
    "\n",
    "A quick note - you should expect to see some errors when going through this workflow. Indeed errors are a normal part of programming in any language! The FDM tools have been designed to be (relatively) robust to various missteps, so you should see an informative error message if you something one of the tools doesn't like. At any rate, if you see an error here, don't panic - it's supposed to be there. Just give the error message a quick read (the main message will be at the bottom of the error readout) and continue with the tutorial - it will (hopefully) keep you abreast of what's happening. \n",
    "\n",
    "We'll start, as we did before, by building each of the individual tables in our test datset. Let's start with `test_table_1`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f9a6f-4c17-4b17-a2ca-506fe7270bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_1 = FDMTable(\n",
    "    source_table_id = \"CY_FDM_BUILDER_TESTS.test_table_1\",\n",
    "    dataset_id = DATASET_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e31775-5f6c-4d47-bdb3-26da74c02f1d",
   "metadata": {},
   "source": [
    "let's take a quick look at the data in `test_table_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb24bd7-2791-48e3-8e77-2eb24f327be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4eb93-70dd-43aa-bb7b-fe1290f43510",
   "metadata": {},
   "source": [
    "and there's our first error! You'll remember that we need a copy of our source dataset in our working datset, otherwise we cann't do anything with it. If you don't, reading the error message above should hopefully serve as a reminder. Let's make a copy and try that again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4a800-7912-4a86-bcf7-44a00a7350c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_1.copy_table_to_dataset()\n",
    "test_table_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c6d101-e603-4950-94ba-2a0e552643f5",
   "metadata": {},
   "source": [
    "Great. `test_table_1` looks like it's already in a perfect state to complete the FDM build process. We'll have our first go with the `quick_build` method. Remember, we just need to point the method to the column(s) that contain the start dates and, if necessary, the end dates. In this case, there's just the one column that contains start dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336ca35-4354-465e-ada9-0c7e3e036f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_1.quick_build(\n",
    "    fdm_start_date_cols=\"start_date\",\n",
    "    fdm_start_date_format=\"DMY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61684b7-0dcd-45b8-968e-6a6fa71c1f8f",
   "metadata": {},
   "source": [
    "and that's the first table built. You can see, the quick build method still gives you some guidance as to what's happening - it's just a lot less verbose and fiddly than the basic `build` method.\n",
    "\n",
    "Now we'll move on to `test_table_2`. As before, we'll initialise an `FDMTable` for `test_table_2`, copy the source data across to our dataset, and take a look at what it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44788dd-4f70-4178-80a2-cff6eac78ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_2 = FDMTable(\n",
    "    source_table_id = \"CY_FDM_BUILDER_TESTS.test_table_2\",\n",
    "    dataset_id = DATASET_ID\n",
    ")\n",
    "test_table_2.copy_table_to_dataset()\n",
    "test_table_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4569a-7fbf-40d5-a4c4-695970685269",
   "metadata": {},
   "source": [
    "You might already notice an issue that might make the build process difficult. If not, no worries - the `quick_build` method is pretty good at pointing them out to you. \n",
    "\n",
    "Let's have a go at building `test_table_2`. This time we have a start and end date, with years and months in separate columns, and no day information. You'll remember we tell the `FDMBuilder` how to find these dates by putting them in a list, and we can use static values for any missing years/months/days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b69a30c-3770-4880-a815-40af6f40ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_2.quick_build(\n",
    "    fdm_start_date_cols = [\"start_year\", \"start_month\", \"15\"],\n",
    "    fdm_start_date_format = \"YMD\",\n",
    "    fdm_end_date_cols = [\"end_year\", \"end_month\", \"15\"],\n",
    "    fdm_end_date_format = \"YMD\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff43e86f-c087-44bb-9143-4c282c692838",
   "metadata": {},
   "source": [
    "As suspected, the `quick_build` process can't find an identifier column. There are digests to be found in `test_table_2`, they're just under a different name `wrong_digest`. We'll rename that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8360c02-3a9e-4bcf-95f1-888cc4c43bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_2.rename_columns({\"wrong_digest\":\"digest\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249867b7-75e9-4853-85f3-2d3b20547293",
   "metadata": {},
   "source": [
    "... and then have another go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c7807-fd77-4ab0-becb-eb08a73d7868",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_2.quick_build(\n",
    "    fdm_start_date_cols = [\"start_year\", \"start_month\", \"15\"],\n",
    "    fdm_start_date_format = \"YMD\",\n",
    "    fdm_end_date_cols = [\"end_year\", \"end_month\", \"15\"],\n",
    "    fdm_end_date_format = \"YMD\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c307e19-8949-43b2-b8d3-2cacbd34f098",
   "metadata": {},
   "source": [
    "fab.\n",
    "\n",
    "Lastly we have `test_table_3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa95b7ba-82fa-42f4-9086-f24294559bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3 = FDMTable(\n",
    "    source_table_id = \"CY_FDM_BUILDER_TESTS.test_table_3\",\n",
    "    dataset_id = DATASET_ID\n",
    ")\n",
    "test_table_3.copy_table_to_dataset()\n",
    "test_table_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7f70c-1b54-49e7-8dd4-882d2486526f",
   "metadata": {},
   "source": [
    "this one looks a little more tricky! You've probably noticed that:\n",
    "\n",
    "* there isn't a recognised identifier in this table\n",
    "* the start and end dates seem to be stored in one column in quite an unhelpful format\n",
    "\n",
    "The lack of id column should hopefully now be an easy one to fix - the \"education_references\" are actually EDRNs, so we just need to rename that column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b23d67-a925-4f8e-99eb-784214026bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3.rename_columns({\"education_reference\":\"EDRN\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272fa793-9386-43c4-88a6-a7742c4ff543",
   "metadata": {},
   "source": [
    "The dates are a little more tricky. We could really do to split up all the information in that one column into separate columns. The `add_column` helper is our friend here, along with the string functions in the BigQuery SQL documentaion:\n",
    "\n",
    "https://cloud.google.com/bigquery/docs/reference/standard-sql/string_functions\n",
    "\n",
    "We can use the `SPLIT` SQL function to split a string into a list, splitting at a specified character. Let's use the `add_column` method to try splitting the `examination_period` at the \"-\" character and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd31bfa3-7515-424f-92d0-7ab901509c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3.add_column(\"SPLIT(examination_period, '-') AS split_exam_period\")\n",
    "test_table_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7143725b-76d9-4c25-9552-f42afe8e7b83",
   "metadata": {},
   "source": [
    "That seems to be a move in the right direction, but we could really use the start and end date information in separate columns - we can use the `OFFSET` SQL function to select a specific item in a list like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e806d0f-806f-422a-a49b-4f00dbe6da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3.add_column(\"split_exam_period[OFFSET(0)] AS start_date\")\n",
    "test_table_3.add_column(\"split_exam_period[OFFSET(1)] AS end_date\")\n",
    "test_table_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3399db21-1e8f-40b3-a51e-3f74dd281c35",
   "metadata": {},
   "source": [
    "That's more like it. We might now decide to get rid of our `split_exam_period` column to make things a little cleaner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf60e72-b5ae-4571-ad0e-c122a9a677ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3.drop_column(\"split_exam_period\")\n",
    "test_table_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5906a75-4846-402a-842b-96cf9b008f76",
   "metadata": {},
   "source": [
    "We still have an issue - the start and end dates aren't full dates in their own right. We either need to add a day to each of the dates, or further divide the start and end into year and month columns. The former option is a bit fiddly, so lets split the `start_date` and `end_date` columns into `start_year`/`start_month`/`end_year`/`end_month` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343ded6-d9a1-46b4-9110-cd4ec5ea44f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3.add_column(\"SPLIT(start_date, '/')[OFFSET(0)] AS start_month\")\n",
    "test_table_3.add_column(\"SPLIT(start_date, '/')[OFFSET(1)] AS start_year\")\n",
    "test_table_3.add_column(\"SPLIT(end_date, '/')[OFFSET(0)] AS end_month\")\n",
    "test_table_3.add_column(\"SPLIT(end_date, '/')[OFFSET(1)] AS end_year\")\n",
    "test_table_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f5f21c-f63c-4d05-a82a-2a934f4b5ade",
   "metadata": {},
   "source": [
    "Great! That's everything we need to build our FDMTable - so let's do just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f56282-9873-485e-9768-8c3a947c742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3.quick_build(\n",
    "    fdm_start_date_cols = [\"start_year\", \"start_month\", \"15\"],\n",
    "    fdm_start_date_format = \"YMD\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc0345-aa8d-4588-b335-f8a1743fa720",
   "metadata": {},
   "source": [
    "You might have noticed we left out the end date columns there. That was deliberate, to demonstrate that the `quick_build` process can be run iteratively. If you forget something, just re-run `quick_build` with the required ammendments and it will pick up where it left off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fa740-1fed-4f7b-b9cb-f6e4dccb8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3.quick_build(\n",
    "    fdm_start_date_cols = [\"start_year\", \"start_month\", \"15\"],\n",
    "    fdm_start_date_format = \"YMD\",\n",
    "    fdm_end_date_cols = [\"end_year\", \"end_month\", \"15\"],\n",
    "    fdm_end_date_format=\"YMD\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f8080-88fb-4678-b4dc-aa8df2ca1d04",
   "metadata": {},
   "source": [
    "You'll note the `quick_build` script keeps any existing work it's already done - so the `person_id` and `fdm_start_date` columns were kept as they are. If you wanted the script to re-build either of these columns, just drop them and then re-start the process again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab8bf4-ea04-4a9e-80ab-ec023134fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_3.drop_column(\"person_id\")\n",
    "test_table_3.quick_build(\n",
    "    fdm_start_date_cols = [\"start_year\", \"start_month\", \"15\"],\n",
    "    fdm_start_date_format = \"YMD\",\n",
    "    fdm_end_date_cols = [\"end_year\", \"end_month\", \"15\"],\n",
    "    fdm_end_date_format=\"YMD\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e02bf-0280-4330-bfc0-de8b8f5a6bb9",
   "metadata": {},
   "source": [
    "And with that, all of the tables have been built, and we're ready to build the FDMDataset. This bit is exactly the same as it was when we covered it in the basics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d217fd-9b17-43eb-bdea-e3b0ffc5a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FDMDataset(\n",
    "    dataset_id = DATASET_ID\n",
    ")\n",
    "dataset.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420a3ab5-cfd3-4258-b766-3d9e15cb075b",
   "metadata": {},
   "source": [
    "It seems like there were a good few issues with each of the tables - let's take a look some of the problems from `test_table_1`. The `FDMTable` object for `test_table_1` will now only point to the non-problem entries, so we'll have to use plain old SQL to look at the problem table:\n",
    "\n",
    "(you'll need to replace YOUR DATASET HERE with your dataset_id - unfortunately python can't be used in `%%bigquery` cell magics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4079f6a-f8ba-47a0-8a8d-e848a80c4275",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT * FROM `CY_SAM_TEST.test_table_1_fdm_problems`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34a03fc-6e8c-4b49-a1eb-7d1f0755c2cb",
   "metadata": {},
   "source": [
    "Let's say, for example, that we know the missing fdm_start_dates were caused by a problem on 15th Jaunary 2020. We could correct that and recover the entries that were removed. \n",
    "\n",
    "To do that, we first have to `recombine` `test_table_1`, that's easily done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730d2c3-489a-4442-9df6-d37d6f4f60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_1.recombine()\n",
    "test_table_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96732f81-fe4e-4b4d-993d-cb435b7b8ef7",
   "metadata": {},
   "source": [
    "The entries in `test_table_1_fdm_problems` have now been added back into `test_table_1`, but the problems column has been retained to make any corrections easier to carry out. We can then correct our null entries. Unfortunatley the FDM tools aren't sophisticated enough to carry out this sort of operation, but we can use SQL again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da6776-eead-49d7-965f-a50afbf221de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "UPDATE `yhcr-prd-phm-bia-core.CY_SAM_TEST.test_table_1`\n",
    "SET start_date = \"15-January-2020\"\n",
    "WHERE fdm_problem = \"Entry has no fdm_start_date\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01afd407-2361-45c6-b0ed-a92ddbf494e8",
   "metadata": {},
   "source": [
    "Now, we'll need to re-build the `fdm_start_date` column so it contains the new dates we've added. To do that, we drop the existing `fdm_start_date` column and then re-run `build` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa3138-669c-4841-b6b5-13e753a38bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_1.drop_column(\"fdm_start_date\")\n",
    "test_table_1.quick_build(\n",
    "    fdm_start_date_cols=\"start_date\",\n",
    "    fdm_start_date_format=\"DMY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74650bd6-1247-4908-bbb1-eae636b71e8b",
   "metadata": {},
   "source": [
    "and then we can re-build the FDM dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771661a-e072-48b1-b9dd-75035c3af334",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8362ab-55ed-447e-aa05-dd8e6ca3ef7f",
   "metadata": {},
   "source": [
    "That about does it for this tutorial. Hopefully you can see how the various tools and helpers we've introduced here can be strung together to make a pretty clean workflow, that'll speed up developing an FDM even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd118990-a074-4c26-abc6-2faae7b3f0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "r-cpu.4-1.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-1:m90"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
